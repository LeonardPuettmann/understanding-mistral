{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "# Set up API URL and headers\n",
    "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "if not api_key: \n",
    "    api_key = input(Fore.YELLOW + \"Please provide your HuggingFace API key: \")\n",
    "headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "# Define functions for streaming and querying the API\n",
    "def stream_response(response):\n",
    "    for line in response.iter_lines():\n",
    "        if line:  # filter out keep-alive new lines\n",
    "            yield json.loads(line)\n",
    "\n",
    "def query(payload):\n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=headers, json=payload, stream=True)\n",
    "        response.raise_for_status()  # raise an exception if the API returned an error\n",
    "        return stream_response(response)\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"An error occurred: {err}\")\n",
    "        return None\n",
    "\n",
    "# Define the chat function and the chat history\n",
    "def chat(message):\n",
    "    global chat_history\n",
    "\n",
    "    # Add the new message to the chat history\n",
    "    chat_history += f\"\\n[INST] {message} [/INST]\"\n",
    "\n",
    "    # Make the API call\n",
    "    output = query({\n",
    "        \"past_user_inputs\": chat_history,\n",
    "        \"inputs\": f\"[INST] {message} [/INST]\",\n",
    "        \"parameters\": {\n",
    "            \"return_full_text\": False,\n",
    "            \"temperature\": 0.01,\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"stream_output\": True\n",
    "        }\n",
    "    })\n",
    "\n",
    "    if output is None:\n",
    "        return \"An error occurred. Please try again later.\"\n",
    "\n",
    "    response_message = \"\"\n",
    "    for data in output:\n",
    "        response_message += data[0][\"generated_text\"]\n",
    "        yield response_message\n",
    "\n",
    "    # Update the chat history with the AI's response\n",
    "    chat_history += f\"\\n{response_message}\"\n",
    "\n",
    "# Initialize chat history (this is kind the system prompt)\n",
    "chat_history = \"[INST] You are an AI-Assistant called Mistral. Be friendly and helpful. Keep your answer short! [/INST]\"\n",
    "\n",
    "# Define the function for printing characters in chunks\n",
    "def print_characters_in_chunks(text, chunk_size=3):\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        print(text[i:i + chunk_size], end=\"\", flush=True)\n",
    "        time.sleep(0.05)\n",
    "\n",
    "# Main loop for the chat interface\n",
    "print(Fore.YELLOW + \"Welcome to Mistral AI!\" + Style.RESET_ALL + \" Type 'quit' to end the conversation.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(Fore.RED + \"\\nYou: \" + Style.RESET_ALL)\n",
    "\n",
    "    if user_input.lower() == \"quit\":\n",
    "        print(Fore.RED + \"Goodbye!\" + Style.RESET_ALL)\n",
    "        break\n",
    "\n",
    "    if len(user_input) > 256:\n",
    "        print(\"Your message is too long. Please limit it to 256 characters.\")\n",
    "        continue\n",
    "\n",
    "    response = chat(user_input)\n",
    "    final_response = \"\"\n",
    "    for partial_response in response:\n",
    "        final_response = partial_response\n",
    "    print(Fore.YELLOW + \"Mistral: \" + Style.RESET_ALL, end=\"\", flush=True)\n",
    "    print_characters_in_chunks(final_response)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
