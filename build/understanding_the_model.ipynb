{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import fire\n",
    "import safetensors.torch\n",
    "import torch\n",
    "from mistral_common.tokens.tokenizers.base import Tokenizer\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "from simple_parsing.helpers import Serializable\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class ModelArgs: \n",
    "    dim: int\n",
    "    n_layers: int\n",
    "    head_dim: int\n",
    "    hidden_dim: int\n",
    "    n_heads: int \n",
    "    n_kv_hads: int \n",
    "    sliding_window: int \n",
    "    norm_eps: float \n",
    "    vocab_size: int \n",
    "    \n",
    "    max_batch_size: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(key: torch.Tensor, values: torch.Tensor, repeasts: int):\n",
    "    keys = torch.repeat_interleave(keys, repeasts=repeasts, dim=2)\n",
    "    values = torch.repeat_interleave(values, repeasts=repeasts, dim=2)\n",
    "    return keys, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unserstanding rotary embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    freqs_cis: complex - (seq_len, head_dim / 2)\n",
    "    x: complex - (bsz, seq_len, head_dim / 2)\n",
    "    \"\"\"\n",
    "    ndim = x.ndim\n",
    "    assert 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1]), ( \n",
    "        freqs_cis.shape,\n",
    "        (x.shape[1], x.shape[-1]),\n",
    "    )\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(\n",
    "        xq: torch.Tensor,\n",
    "        xk: torch.Tensor,\n",
    "        freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "\n",
    "    freqs_cis = _reshape_for_broadcast(freqs_cis, xq_)\n",
    "\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.args = args \n",
    "        \n",
    "        self.n_heads: int = args.n_heads\n",
    "        self.n_kv_heads: int = args.n_kv_heads\n",
    "        \n",
    "        self.repeats =self.n_heads // self.n_kv_heads\n",
    "        self.sliding_window = self.args.sliding_window\n",
    "        \n",
    "        self.scale = self.args.head_dim**-0.5\n",
    "        \n",
    "        # Instantiate the weight matrices for query, key, value and \n",
    "        self.wq = nn.Linear(\n",
    "            args.dim, # 4096\n",
    "            args.n_heads * args.head_dim, # 32 * 128 = 4.096\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        self.wk = nn.Linear(\n",
    "            args.dim, # 4096\n",
    "            args.n_heads * args.head_dim, # 32 * 128 = 4.096\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        self.wv = nn.Linear(\n",
    "            args.dim, # 4096\n",
    "            args.n_heads * args.head_dim, # 32 * 128 = 4.096\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        # weight output\n",
    "        self.wo = nn.Linear(\n",
    "            args.n_heads * args.head_dim,\n",
    "            args.dim, \n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        self.cache_k = torch.emtpy(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.sliding_window,\n",
    "                self.n_kv_heads,\n",
    "                self.args.head_dim,\n",
    "            ), dtype=torch.float16\n",
    "        ).cuda()\n",
    "        \n",
    "        self.cache_v = torch.emtpy(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.sliding_window,\n",
    "                self.n_kv_heads,\n",
    "                self.args.head_dim,\n",
    "            ), dtype=torch.float16\n",
    "        ).cuda()\n",
    "        \n",
    "        \n",
    "    def forward(\n",
    "        self, x: torch.Tensor, freqs_cis: torch.Tensor, position: torch.Tensor, mask: Optional[torch.Tensor]\n",
    "    ) -> torch.Tensor: \n",
    "        \n",
    "        bsz, seqlen, _ = x.shape\n",
    "        \n",
    "        xq, xK, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "        xq = xq.view(bsz, seqlen, self.n_heads, self.args.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_kv_heads, self.args.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_kv_heads, self.args.head_dim)\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "        \n",
    "        # The cache is a rotating buffer\n",
    "        scatter_pos = (position[-self.sliding_window:] % self.sliding_window)[None, :, None, None]\n",
    "        scatter_pos = scatter_pos.repeat(bsz, 1, self.n_kv_heads, self.args.head_dim)\n",
    "        self.cache_k[:bsz].scatter_(dim=1, index=scatter_pos, src=xk[:, -self.sliding_window:])\n",
    "        self.cache_v[:bsz].scatter_(dim=1, index=scatter_pos, src=xv[:, -self.sliding_window:])\n",
    "        \n",
    "        if position.shape[0] > 1:\n",
    "            key, value = repeat_kv(xk, xv, self.repeats)\n",
    "        else: \n",
    "            cur_pos = position[-1].item() + 1\n",
    "            key, value = repeat_kv(self.cache_k[:bsz, :cur_pos, ...], self.cache_v[:bsz, :cur_pos, ...], self.repeats)\n",
    "            \n",
    "        query = xq.transpose(1, 2)\n",
    "        key = key.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(query, key.transpose(2, 3)) * self.scale\n",
    "        \n",
    "        if mask is not None: \n",
    "            scores += mask[None, None, ...]\n",
    "            \n",
    "        scores = scores.float()\n",
    "        scores = nn.functional.softmax(scores, dim=-1).type_as(query)\n",
    "        output = torch.matmul(scores, value)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        return self.wo(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.w1 = nn.Linear(args.dim, args.hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(args.hidden_dim, args.dim, bias=False)\n",
    "        self.w3 = nn.Linear(args.dim, args.hidden_dim, bias=False)\n",
    "        \n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Root Mean Square Layer Norm. See here for more: https://arxiv.org/abs/1910.07467\n",
    "-> RMSNorm regularizes the summed inputs to a neuron in one layer according to root mean square (RMS), giving the model re-scaling invariance property and implicit learning rate adaptation ability. RMSNorm is computationally simpler and thus more efficient than LayerNorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __inut__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps \n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        \n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module): \n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.args = args \n",
    "        self.vocab_size = args.vocab_size\n",
    "        self.n_layers = args.n_layers \n",
    "        assert self.vocab_size > 0 \n",
    "        \n",
    "        self.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)\n",
    "        \n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [TransformerBlock(args=args) for _ in range(args.n_layers)]\n",
    "        )\n",
    "        \n",
    "        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        \n",
    "        self.output = nn.Linear(args.dim, args.vocab_size, bias=False)\n",
    "        \n",
    "        theta = self.args.rope_theta or 1000000.0\n",
    "        self.freqs_cis = precompute_freqs_cis(self.args.head_dim, 128_000, theta).to(\n",
    "            \"cuda\"\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        positions: torch.Tensor\n",
    "    ):\n",
    "        h = self.tok_embeddings(input_ids)\n",
    "        freqs_cis = self.freqs_cis[positions]\n",
    "        \n",
    "        mask: Optional[torch.Tensor] = None \n",
    "        if input_ids.shape[1] > 1:\n",
    "            seqlen = input_ids.shape[1]\n",
    "            tensor = torch.full(\n",
    "                (seqlen, seqlen),\n",
    "                dtype=h.dtype,\n",
    "                fill_value=1,\n",
    "                device=h.device\n",
    "            )\n",
    "            mask = torch.tril(tensor, diagonal=0).to(h.dtype)\n",
    "            mask = torch.triu(mask, diagonal=-self.args.max_seq_len)\n",
    "            mask = torch.log(mask)\n",
    "            \n",
    "        for layer in self.layers: \n",
    "            h = layer(h, freqs_cis, positions, mask)\n",
    "            \n",
    "        return self.output(self.norm(h)).float()\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_folder(\n",
    "        folder: Path, max_batch_size: int=1, device=\"cuda\", dtype=torch.float16\n",
    "    ):\n",
    "        with open(Path(folder) / \"params.json\", \"r\") as f:\n",
    "            model_args = ModelArgs.from_dict(json.load(f))\n",
    "        model_args.max_batch_size = max_batch_size \n",
    "        \n",
    "        model = Transformer(model_args)\n",
    "        \n",
    "        pt_model_file = Path(folder) / \"consolidated.00.pth\"\n",
    "        safetensors_model_file = Path(folder) / \"consolidated.safetensors\"\n",
    "        \n",
    "        assert (\n",
    "            pt_model_file.exists() or safetensors_model_file.exists()\n",
    "        ), f\"Make sure either {pt_model_file} or {safetensors_model_file} exists\"\n",
    "        assert not (\n",
    "            pt_model_file.exists() and safetensors_model_file.exists()\n",
    "        ), f\"Both {pt_model_file} and {safetensors_model_file} cannot exist\"\n",
    "\n",
    "        if pt_model_file.exists():\n",
    "            loaded = torch.load(str(pt_model_file), mmap=True)\n",
    "        else:\n",
    "            loaded = safetensors.torch.load_file(str(safetensors_model_file))\n",
    "\n",
    "        model.load_state_dict(loaded, assign=True, strict=True)\n",
    "        return model.to(device=device, dtype=dtype)\n",
    "    \n",
    "    def load_tokenizer(model_path: Path) -> MistralTokenizer:\n",
    "        tokenizer = [\n",
    "            f for f in os.listdir(Path(model_path)) if f.startswith(tokenizer.model)\n",
    "        ]\n",
    "        \n",
    "        assert (\n",
    "            len(tokenizer) > 0\n",
    "        ), f\"No tokenizer found in {model_path}, make sure to place a `tokenizer.model.[v1,v2,v3]` file in {model_path}.\"\n",
    "        \n",
    "        assert (\n",
    "            len(tokenizer) == 1\n",
    "        ), f\"Multiple tokenizers {', '.join(tokenizer)} found in `model_path`, make sure to only have one tokenizer\"\n",
    "\n",
    "        tokenizer = MistralTokenizer.from_file(str(model_path / tokenizer[0]))\n",
    "        \n",
    "        return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to generate text, we need a dedicated function to do so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    prompts: List[str], model: Transformer, tokenizer: Tokenizer, max_tokens: int\n",
    "):\n",
    "    encoded_prompts = [\n",
    "        tokenizer.encode(prompt, bos=True, eos=False) for prompt in prompts\n",
    "    ]\n",
    "    prompt_lens = [len(x) for x in encoded_prompts]\n",
    "    min_prompt_len = min(prompt_lens)\n",
    "    max_prompt_len = max(prompt_lens)\n",
    "\n",
    "    input_tokens = torch.full(\n",
    "        (len(prompts), max_prompt_len), tokenizer._model.pad_id(), dtype=\"cuda\"\n",
    "    )\n",
    "\n",
    "    for i, encoded in enumerate(encoded_prompts):\n",
    "        input_tokens[i, : len(encoded)] = torch.tensor(encoded).to(input_tokens)\n",
    "\n",
    "    input_mask = input_tokens != tokenizer._model.pad_id()\n",
    "\n",
    "    # pre-fill\n",
    "    positions = torch.arange(0, min_prompt_len).to(\"cuda\")\n",
    "    logits = model.forward(input_tokens[:, :min_prompt_len], positions)\n",
    "    logprobs = nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # decode \n",
    "    generated = []\n",
    "    all_logprobs = [\n",
    "        logprobs[:, :-1, :]\n",
    "        .gather(2, input_tokens[:, 1:min_prompt_len, None])\n",
    "        .squeeze(-1)\n",
    "    ]\n",
    "    cur_pos = min_prompt_len\n",
    "    for _ in range(max_tokens):\n",
    "        next_token = torch.argmax(logprobs[:, -1, :], dim=-1)\n",
    "        if cur_pos < input_mask.shape[1]:\n",
    "            next_token = torch.where(\n",
    "                input_mask[:, cur_pos], input_tokens[:, cur_pos], next_token\n",
    "            )\n",
    "        all_logprobs.append(\n",
    "            logprobs[:, -1, :].gather(1, next_token[:, None])\n",
    "        )\n",
    "        generated.append(next_token[:, None])\n",
    "        logits = model.forward(\n",
    "            next_token[:, None], torch.LongTensor([cur_pos]).to(next_token)\n",
    "        )\n",
    "        logprobs = nn.functional.log_softmax(logits, dim=-1)\n",
    "        cur_pos += 1\n",
    "\n",
    "    all_logprobs = torch.cat(all_logprobs, dim=1)\n",
    "    res = []\n",
    "    if max_tokens > 0: \n",
    "        generated = torch.cat(generated, 1)\n",
    "\n",
    "        for i, x in enumerate(encoded_prompts):\n",
    "            res.append(tokenizer.decode(x[:min_prompt_len] + generated[i].tolist()))\n",
    "    return res, all_logprobs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
