{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leopu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "# Third-party library imports\n",
    "import fire\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "from datasets import load_dataset\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from mistral_common.protocol.instruct.messages import (\n",
    "    UserMessage,\n",
    ")\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.protocol.instruct.tool_calls import (\n",
    "    Function,\n",
    "    Tool,\n",
    ")\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "\n",
    "# Local application/library-specific imports\n",
    "from mistral import ModelArgs, Transformer, RMSNorm, precompute_freqs_cis, generate\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset in streaming mode\n",
    "ds = load_dataset(\"HuggingFaceTB/cosmopedia\", \"stories\", streaming=True,)\n",
    "\n",
    "# Initialize a counter\n",
    "counter = 0\n",
    "\n",
    "# Iterate over the dataset\n",
    "dataset = {\n",
    "    \"text\": [],\n",
    "}\n",
    "\n",
    "for sample in ds[\"train\"]:\n",
    "    dataset[\"text\"].append(sample[\"text\"])\n",
    "    counter += 1\n",
    "    if counter >= 10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer outside of the function\n",
    "tokenizer = MistralTokenizer.v1()\n",
    "\n",
    "def tokenize_text(text, tokenizer, return_text=False):\n",
    "    # Tokenize the input text using the provided tokenizer\n",
    "    tokenized = tokenizer.encode_chat_completion(\n",
    "        ChatCompletionRequest(\n",
    "            messages=[UserMessage(content=text)],\n",
    "            model=\"open-mistral-7b\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    tokens = tokenized.tokens\n",
    "    tokenized_text = tokenized.text\n",
    "\n",
    "    if return_text:\n",
    "        return tokens, tokenized_text\n",
    "    else:\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Once upon a time, in a village called Kiwiland, there lived two best friends named Kiwi and Koala. They loved exploring the world around them and learning new things every day! One day, they stumbled upon a magical forest full of vibrant colors and fascinating creatures. As they ventured deeper into the forest, they met Torty, a wise old turtle who was known to have answers to all questions.\\n\\nKiwi asked Torty, \"How does our culture affect the way we make decisions?\" Torty smiled and replied, \"Well my dear friend, let me tell you a story.\"\\n\\nLong ago, in another part of the forest, there were two tribes - the Hares and the Sloths. The Hares valued speed and quickness, believing that swift actions led to success. On the other hand, the Sloths cherished patience and deliberation, thinking that slow yet thoughtful decisions brought prosperity.\\n\\nOne sunny afternoon, both tribes faced a challenge – sharing a limited supply of fruits between them. The Hares wanted to divide the fruits quickly so they could move on to their next adventure. Meanwhile, the Sloths preferred taking ample time to ensure fairness and satisfaction among everyone involved. Their contrasting approaches caused tension and disagreements until they realized something important.\\n\\nTogether, they discovered that neither rushing nor delaying decisions guaranteed perfection. Instead, finding a balance between haste and caution proved essential. By respecting each other\\'s perspectives, they created harmony and shared resources wisely. Since then, these neighboring tribes learned valuable lessons about embracing diversity and understanding cultural differences when making decisions together.\\n\\n\"Ah!\" said Kiwi excitedly, \"So acknowledging various viewpoints helps us make better decisions.\" Torty nodded approvingly before adding one final piece of wisdom.\\n\\n\"Yes, indeed,\" he continued. \"But remember, even after considering diverse opinions, sometimes outcomes won\\'t turn out as desired. That\\'s when resilience comes in handy—learning from those experiences and moving forward positively teaches us humility and growth.\"\\n\\nWith wide eyes filled with curiosity, Koala wondered aloud, \"What happens if someone still isn\\'t happy despite trying hard?\" Smiling softly, Torty answered, \"That\\'s life\\'s unexpected twist, dear Koala. We must accept it graciously because failure paves the pathway towards improvement and progress.\"',\n",
       " ' In a bustling town full of curious creatures called the \"Wordlings,\" there lived two friends named Exy and Uppe. They loved exploring their world made up of different kinds of words. One sunny day, they decided to play a game of finding words that fit specific criteria. Their teacher, Professor Vowel, challenged them to create a magical tool that would filter out words containing the letter \\'e\\', starting with a capital letter, and no more than five letters long. Not only did it need to show these special words, but it had to count how many were found too!\\n\\nAs they began working on their project, Exy asked, \"Why do we only want words without the letter \\'e\\'?\" Uppe replied, \"Well, my dear friend, think about our Wordling language. We use vowels like \\'e\\' to make softer sounds when we speak or read. Words without \\'e\\' might give us stronger meanings!\"\\n\\nWith excitement in their eyes, they continued building their enchanted tool. Finally, after hours of tinkering, they held something extraordinary - a device that took in all sorts of words and presented only those meeting the requirements! It even displayed the number of chosen words, right beside the newly formed list.\\n\\nExy and Uppe ran off to share their invention with everyone. As the Wordlings gathered around, marveling at its power, suddenly, the unexpected happened. A gust of wind swept across the square carrying away some of the precious words! Everyone watched helplessly as the whirlwind scattered them into oblivion. Although saddened by what transpired, they learned a valuable lesson about preservation and importance of every single word in their lives. Even short ones mattered equally because together, they created meaningful stories and messages among the Wordlings.',\n",
       " \" Step 3: Embracing an Unconventional Warmup Routine\\n\\nAs Maria continued her warmup, she noticed a peculiar group of people gathering nearby - they were practicing something called 'parkour.' Intrigued, she decided to approach them and learn more about their unusual activity. To her surprise, they welcomed her warmly and invited her to join them for a session.\\n\\nBeing open-minded and always eager to try new things, Maria accepted their offer and quickly discovered that parkour offered a unique full-body warmup experience unlike anything she had ever done before. It involved running, jumping, climbing, and vaulting over various obstacles – all skills that required strength, flexibility, agility, and coordination. As she practiced alongside these enthusiasts, she found herself laughing, learning, and getting an incredible adrenaline rush from pushing her limits.\\n\\nStep 4: Adapting Her Fitness Journey\\n\\nAfter incorporating parkour into her pre-jog routine, Maria realized that not only did it provide a fantastic warmup, but it also added variety and excitement to her fitness journey. Moreover, she met many interesting individuals who shared her passion for staying fit and exploring unconventional ways to do so.\\n\\nOne day, while discussing her experiences with fellow Reddit users on r/Fitness, Maria encountered another surprising turn of events. A professional parkour athlete recognized her enthusiasm and dedication and reached out to her through a private message. They expressed admiration for Maria's determination and commitment to improving her health and wellness and extended an invitation for her to participate in local competitions.\\n\\nStep 5: Joining the Parkour Community\\n\\nWith some encouragement from her newfound friend and mentor, Maria embarked on a thrilling adventure within the world of competitive parkour. Through rigorous training sessions and countless hours spent mastering new techniques, she developed confidence and resilience both physically and mentally. Alongside other passionate athletes, Maria formed lasting bonds and created cherished memories.\\n\\nReflecting on her transformation, Maria acknowledged that embracing her curiosity and stepping outside of her comfort zone led to discovering an enriching niche interest. By sharing her journey online, she hoped to inspire others facing similar challenges to explore unfamiliar territories and embrace change wholeheartedly.\\n\\nIn conclusion, sometimes life takes us down unexpected paths when we least expect it, just like Maria's encounter with parkour enthusiasts during her usual jogging route. These moments of serendipity often result in profound connections and growth opportunities worth pursuing. So go ahead; take risks, engage in conversations, and never shy away from trying something new – your next great adventure might be waiting around the corner!\",\n",
       " ' Once upon a time, in a small town named Harmonyville, lived three friends - Sammy the Sunflower, Billy the Breeze, and George the Green Tree. They were not ordinary plants; they had feelings and could talk! These three friends loved their town and played together every day. However, one day they noticed something strange happening around them. It was getting hotter than usual during summer, winters weren\\'t so cold anymore, and sometimes it even rained when it shouldn\\'t. This worried our little friends deeply.\\n\\nSammy, being bright and curious, decided to ask Mr. Wise Owl, the oldest creature in Harmonyville, about these changes. \"Mr. Wise Owl,\" asked Sammy, \"why has our weather become so unpredictable?\" Mr. Wise Owl sighed before answering, \"Dear Sammy, humans burn too much coal, oil, and gas for electricity, heat, and transportation which releases gases into Earth\\'s atmosphere trapping more sunlight causing temperatures to rise.\"\\n\\n\"Oh no!\" cried out Billy and George joining the conversation. Mr. Wise Owl continued, \"But don\\'t worry my young friends, we can help by showing the humans how to use other forms of energy like solar, wind power, and water current instead of burning fuels.\" \\n\\nWith newfound hope, Sammy, Billy, and George took action. With the guidance of Mr. Wise Owl, they started spreading knowledge about alternative energy sources and encouraged everyone to save energy wherever possible. Many humans listened and made positive changes reducing their carbon footprints. But sadly, despite all efforts, the unusual weather patterns did not completely disappear because there were still many humans who didn\\'t heed the advice.\\n\\nThough disheartened, our brave trio learned valuable lessons on resilience and perseverance. Even if you cannot solve a problem entirely, every effort counts and contributes towards making things better. And importantly, never underestimate the importance of sharing knowledge and working collaboratively to protect our beautiful planet home.',\n",
       " \" On a bright, sunny day, two best friends, Timmy the Gazelle and Sally the Seal, decided to go on an adventure. They wanted to explore their homes of the grassland and ocean, and see which was better - running or swimming.\\n\\nAs they started their journey, Timmy leaped and bounded through the tall, green grass. He exclaimed, “Wow! Running is so much fun! It feels like I am flying!” Sally agreed, “Yes, Timmy, but let me show you how wonderful swimming can be too!” She then dove into the cool, crystal-clear water and began to swim effortlessly. She said, “Swimming is like being one with the water. It makes me feel free and peaceful.”\\n\\nTimmy and Sally spent all day exploring their worlds and comparing their experiences. Timmy marveled at the way Sally moved through the water, almost as if she were part of it. And Sally couldn’t believe how fast and agile Timmy was on land. At lunchtime, they talked about what they had learned. Timmy shared his discovery about friction – how he used it to run quickly by pushing off the ground, leaving small pits behind him. Sally explained buoyancy, saying that even though seals weigh a lot, they float easily because of the water’s support.\\n\\nFinally, they wondered who would win a race combining both running and swimming. A wise old turtle overheard their discussion and suggested organizing a competition among various animals. Everyone gathered near the river, excited and curious. To everyone's surprise, neither Timmy nor Sally won; instead, it was Kiki Koala, who climbed trees swiftly and swam strongly against the current. Though initially disappointed, Timmy and Sally realized something crucial: every creature has unique abilities, making each special and valuable. This unexpected revelation taught them a significant lesson about embracing diversity and celebrating differences in both nature and life.\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"text\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = [tokenize_text(text, tokenizer) for text in dataset[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 733, 16289, 28793, 733, 5992, 28793, 733, 28748, 16289, 28793],\n",
       " '<s>▁[INST]▁[NULL]▁[/INST]')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_text(\"[NULL]\", tokenizer, return_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 733, 16289, 28793, 12014, 733, 28748, 16289, 28793],\n",
       " '<s>▁[INST]▁hi▁[/INST]')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_text(\"hi\", tokenizer, return_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Convert tokenized data into a tensor\n",
    "tokens_tensor = [torch.tensor(tokens) for tokens in tokenized_data]\n",
    "num_tokens = sum([len(tokens) for tokens in tokenized_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5960969\n"
     ]
    }
   ],
   "source": [
    "print(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = max(len(seq) for seq in tokens_tensor)\n",
    "\n",
    "class PaddedSequenceTokenDataset(Dataset):\n",
    "    def __init__(self, sequences, vocab_size, max_seq_len):\n",
    "        self.sequences = sequences\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        # Convert the sequence of tokens to a tensor.\n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.long)\n",
    "        # Pad the sequence with 0s.\n",
    "        padded_sequence = torch.cat((sequence_tensor, torch.zeros(self.max_seq_len - len(sequence), dtype=torch.long)))\n",
    "        return padded_sequence\n",
    "\n",
    "vocab_size = 10000  # This is the size of your vocabulary\n",
    "dataset = PaddedSequenceTokenDataset(tokens_tensor, vocab_size, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1  # This is the size of the batches you want to load\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leopu\\AppData\\Local\\Temp\\ipykernel_17244\\2977917119.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence_tensor = torch.tensor(sequence, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   733, 16289,  ...,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "for batch in data_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new ModelArgs object with the desired configuration\n",
    "model_args = ModelArgs(\n",
    "    dim=512,\n",
    "    n_layers=16,\n",
    "    head_dim=32,\n",
    "    hidden_dim=2048,\n",
    "    n_heads=8,\n",
    "    n_kv_heads=8,\n",
    "    vocab_size=32000,\n",
    "    norm_eps=1e-5,\n",
    "    max_batch_size=3,\n",
    ")\n",
    "\n",
    "# Create a new Transformer object with random weights\n",
    "model = Transformer(model_args).to(\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Transformer                              --\n",
       "├─Embedding: 1-1                         16,384,000\n",
       "├─ModuleList: 1-2                        --\n",
       "│    └─TransformerBlock: 2-1             --\n",
       "│    │    └─Attention: 3-1               524,288\n",
       "│    │    └─FeedForward: 3-2             3,145,728\n",
       "│    │    └─RMSNorm: 3-3                 512\n",
       "│    │    └─RMSNorm: 3-4                 512\n",
       "│    └─TransformerBlock: 2-2             --\n",
       "│    │    └─Attention: 3-5               524,288\n",
       "│    │    └─FeedForward: 3-6             3,145,728\n",
       "│    │    └─RMSNorm: 3-7                 512\n",
       "│    │    └─RMSNorm: 3-8                 512\n",
       "│    └─TransformerBlock: 2-3             --\n",
       "│    │    └─Attention: 3-9               524,288\n",
       "│    │    └─FeedForward: 3-10            3,145,728\n",
       "│    │    └─RMSNorm: 3-11                512\n",
       "│    │    └─RMSNorm: 3-12                512\n",
       "│    └─TransformerBlock: 2-4             --\n",
       "│    │    └─Attention: 3-13              524,288\n",
       "│    │    └─FeedForward: 3-14            3,145,728\n",
       "│    │    └─RMSNorm: 3-15                512\n",
       "│    │    └─RMSNorm: 3-16                512\n",
       "│    └─TransformerBlock: 2-5             --\n",
       "│    │    └─Attention: 3-17              524,288\n",
       "│    │    └─FeedForward: 3-18            3,145,728\n",
       "│    │    └─RMSNorm: 3-19                512\n",
       "│    │    └─RMSNorm: 3-20                512\n",
       "│    └─TransformerBlock: 2-6             --\n",
       "│    │    └─Attention: 3-21              524,288\n",
       "│    │    └─FeedForward: 3-22            3,145,728\n",
       "│    │    └─RMSNorm: 3-23                512\n",
       "│    │    └─RMSNorm: 3-24                512\n",
       "│    └─TransformerBlock: 2-7             --\n",
       "│    │    └─Attention: 3-25              524,288\n",
       "│    │    └─FeedForward: 3-26            3,145,728\n",
       "│    │    └─RMSNorm: 3-27                512\n",
       "│    │    └─RMSNorm: 3-28                512\n",
       "│    └─TransformerBlock: 2-8             --\n",
       "│    │    └─Attention: 3-29              524,288\n",
       "│    │    └─FeedForward: 3-30            3,145,728\n",
       "│    │    └─RMSNorm: 3-31                512\n",
       "│    │    └─RMSNorm: 3-32                512\n",
       "│    └─TransformerBlock: 2-9             --\n",
       "│    │    └─Attention: 3-33              524,288\n",
       "│    │    └─FeedForward: 3-34            3,145,728\n",
       "│    │    └─RMSNorm: 3-35                512\n",
       "│    │    └─RMSNorm: 3-36                512\n",
       "│    └─TransformerBlock: 2-10            --\n",
       "│    │    └─Attention: 3-37              524,288\n",
       "│    │    └─FeedForward: 3-38            3,145,728\n",
       "│    │    └─RMSNorm: 3-39                512\n",
       "│    │    └─RMSNorm: 3-40                512\n",
       "│    └─TransformerBlock: 2-11            --\n",
       "│    │    └─Attention: 3-41              524,288\n",
       "│    │    └─FeedForward: 3-42            3,145,728\n",
       "│    │    └─RMSNorm: 3-43                512\n",
       "│    │    └─RMSNorm: 3-44                512\n",
       "│    └─TransformerBlock: 2-12            --\n",
       "│    │    └─Attention: 3-45              524,288\n",
       "│    │    └─FeedForward: 3-46            3,145,728\n",
       "│    │    └─RMSNorm: 3-47                512\n",
       "│    │    └─RMSNorm: 3-48                512\n",
       "│    └─TransformerBlock: 2-13            --\n",
       "│    │    └─Attention: 3-49              524,288\n",
       "│    │    └─FeedForward: 3-50            3,145,728\n",
       "│    │    └─RMSNorm: 3-51                512\n",
       "│    │    └─RMSNorm: 3-52                512\n",
       "│    └─TransformerBlock: 2-14            --\n",
       "│    │    └─Attention: 3-53              524,288\n",
       "│    │    └─FeedForward: 3-54            3,145,728\n",
       "│    │    └─RMSNorm: 3-55                512\n",
       "│    │    └─RMSNorm: 3-56                512\n",
       "│    └─TransformerBlock: 2-15            --\n",
       "│    │    └─Attention: 3-57              524,288\n",
       "│    │    └─FeedForward: 3-58            3,145,728\n",
       "│    │    └─RMSNorm: 3-59                512\n",
       "│    │    └─RMSNorm: 3-60                512\n",
       "│    └─TransformerBlock: 2-16            --\n",
       "│    │    └─Attention: 3-61              524,288\n",
       "│    │    └─FeedForward: 3-62            3,145,728\n",
       "│    │    └─RMSNorm: 3-63                512\n",
       "│    │    └─RMSNorm: 3-64                512\n",
       "├─RMSNorm: 1-3                           512\n",
       "├─Linear: 1-4                            16,384,000\n",
       "=================================================================\n",
       "Total params: 91,505,152\n",
       "Trainable params: 91,505,152\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated by PyTorch on the GPU: 8564.41 MB\n"
     ]
    }
   ],
   "source": [
    "# Get the number of bytes currently allocated on the GPU\n",
    "allocated_bytes = torch.cuda.memory_allocated()\n",
    "\n",
    "# Convert to megabytes\n",
    "allocated_mb = allocated_bytes / 1_048_576\n",
    "\n",
    "print(f'Memory allocated by PyTorch on the GPU: {allocated_mb:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 91505152\n",
      "Number of buffers: 0\n",
      "Memory for parameters: 349.06 MB\n",
      "Memory for buffers: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_buffers = sum(b.numel() for b in model.buffers())\n",
    "\n",
    "# Convert to bytes\n",
    "params_bytes = num_params * 4\n",
    "buffers_bytes = num_buffers * 4\n",
    "\n",
    "# Convert to megabytes\n",
    "params_mb = params_bytes / 1_048_576\n",
    "buffers_mb = buffers_bytes / 1_048_576\n",
    "\n",
    "print(f'Number of parameters: {num_params}')\n",
    "print(f'Number of buffers: {num_buffers}')\n",
    "print(f'Memory for parameters: {params_mb:.2f} MB')\n",
    "print(f'Memory for buffers: {buffers_mb:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "# Get current date and time\n",
    "now = datetime.datetime.now()\n",
    "date_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Find the next available run number\n",
    "run_number = 0\n",
    "while os.path.exists(f'logs/training-run-{run_number}-{date_time}.log'):\n",
    "    run_number += 1\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename=f'logs/training-run-{run_number}-{date_time}.log', level=logging.INFO)\n",
    "\n",
    "# Your model and data setup here...\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for batch_idx, batch in enumerate(tqdm(data_loader)):\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch.to(device)\n",
    "    positions = torch.arange(0, max_seq_len).to(device)\n",
    "    logits = model.forward(input_ids, positions)\n",
    "    logprobs = nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "    _, predicted = torch.max(logprobs, dim=2)\n",
    "\n",
    "    loss = loss_fn(logprobs.view(-1, logprobs.size(-1)), input_ids.view(-1))\n",
    "    logging.info(f'Batch {batch_idx}, Loss: {loss.item()}')\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Save model checkpoint every 100 batches\n",
    "    if (batch_idx + 1) % 100 == 0:\n",
    "        torch.save(model.state_dict(), f'models/model_checkpoint_run-{run_number}_{date_time}_batch-{batch_idx}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "][ kunumber comparingiverategor counrotejsactic hasn mot worry ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  Pero харак hosp Creativeotimesን龙))\n"
     ]
    }
   ],
   "source": [
    "prompt = \"This is a test prompt \"\n",
    "generated_text, logprobs = generate([prompt], model, tokenizer, max_tokens=35)\n",
    "print(generated_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
