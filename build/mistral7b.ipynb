{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from dataclasses import dataclass \n",
    "from pathlib import Path \n",
    "import fire \n",
    "import json\n",
    "from typing import Optional, Tuple, List \n",
    "from sentencepiec import SentencePieceProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class ModelArgs: \n",
    "    dim: int\n",
    "    n_layers: int\n",
    "    head_dim: int\n",
    "    hidden_dim: int\n",
    "    n_heads: int \n",
    "    n_kv_hads: int \n",
    "    sliding_window: int \n",
    "    norm_eps: float \n",
    "    vocab_size: int \n",
    "    \n",
    "    max_batch_size: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(key: torch.Tensor, values: torch.Tensor, repeasts: int):\n",
    "    keys = torch.repeat_interleave(keys, repeasts=repeasts, dim=2)\n",
    "    values = torch.repeat_interleave(values, repeasts=repeasts, dim=2)\n",
    "    return keys, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unserstanding rotary embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    freqs_cis: complex - (seq_len, head_dim / 2)\n",
    "    x: complex - (bsz, seq_len, head_dim / 2)\n",
    "    \"\"\"\n",
    "    ndim = x.ndim\n",
    "    assert 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1]), ( \n",
    "        freqs_cis.shape,\n",
    "        (x.shape[1], x.shape[-1]),\n",
    "    )\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(\n",
    "        xq: torch.Tensor,\n",
    "        xk: torch.Tensor,\n",
    "        freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "\n",
    "    freqs_cis = _reshape_for_broadcast(freqs_cis, xq_)\n",
    "\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.args = args \n",
    "        \n",
    "        self.n_heads: int = args.n_heads\n",
    "        self.n_kv_heads: int = args.n_kv_heads\n",
    "        \n",
    "        self.repeats =self.n_heads // self.n_kv_heads\n",
    "        self.sliding_window = self.args.sliding_window\n",
    "        \n",
    "        self.scale = self.args.head_dim**-0.5\n",
    "        \n",
    "        # Instantiate the weight matrices for query, key, value and \n",
    "        self.wq = nn.Linear(\n",
    "            args.dim, # 4096\n",
    "            args.n_heads * args.head_dim, # 32 * 128 = 4.096\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        self.wk = nn.Linear(\n",
    "            args.dim, # 4096\n",
    "            args.n_heads * args.head_dim, # 32 * 128 = 4.096\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        self.wv = nn.Linear(\n",
    "            args.dim, # 4096\n",
    "            args.n_heads * args.head_dim, # 32 * 128 = 4.096\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        # weight output\n",
    "        self.wo = nn.Linear(\n",
    "            args.n_heads * args.head_dim,\n",
    "            args.dim, \n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        self.cache_k = torch.emtpy(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.sliding_window,\n",
    "                self.n_kv_heads,\n",
    "                self.args.head_dim,\n",
    "            ), dtype=torch.float16\n",
    "        ).cuda()\n",
    "        \n",
    "        self.cache_v = torch.emtpy(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.sliding_window,\n",
    "                self.n_kv_heads,\n",
    "                self.args.head_dim,\n",
    "            ), dtype=torch.float16\n",
    "        ).cuda()\n",
    "        \n",
    "        \n",
    "    def forward(\n",
    "        self, x: torch.Tensor, freqs_cis: torch.Tensor, position: torch.Tensor, mask: Optional[torch.Tensor]\n",
    "    ) -> torch.Tensor: \n",
    "        \n",
    "        bsz, seqlen, _ = x.shape\n",
    "        \n",
    "        xq, xK, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "        xq = xq.view(bsz, seqlen, self.n_heads, self.args.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_kv_heads, self.args.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_kv_heads, self.args.head_dim)\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "        \n",
    "        # The cache is a rotating buffer\n",
    "        scatter_pos = (position[-self.sliding_window:] % self.sliding_window)[None, :, None, None]\n",
    "        scatter_pos = scatter_pos.repeat(bsz, 1, self.n_kv_heads, self.args.head_dim)\n",
    "        self.cache_k[:bsz].scatter_(dim=1, index=scatter_pos, src=xk[:, -self.sliding_window:])\n",
    "        self.cache_v[:bsz].scatter_(dim=1, index=scatter_pos, src=xv[:, -self.sliding_window:])\n",
    "        \n",
    "        if position.shape[0] > 1:\n",
    "            key, value = repeat_kv(xk, xv, self.repeats)\n",
    "        else: \n",
    "            cur_pos = position[-1].item() + 1\n",
    "            key, value = repeat_kv(self.cache_k[:bsz, :cur_pos, ...], self.cache_v[:bsz, :cur_pos, ...], self.repeats)\n",
    "            \n",
    "        query = xq.transpose(1, 2)\n",
    "        key = key.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(query, key.transpose(2, 3)) * self.scale\n",
    "        \n",
    "        if mask is not None: \n",
    "            scores += mask[None, None, ...]\n",
    "            \n",
    "        scores = scores.float()\n",
    "        scores = nn.functional.softmax(scores, dim=-1).type_as(query)\n",
    "        output = torch.matmul(scores, value)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        return self.wo(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
