{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from dataclasses import dataclass \n",
    "from pathlib import Path \n",
    "import fire \n",
    "import json\n",
    "from typing import Optional, Tuple, List \n",
    "from sentencepiec import SentencePieceProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class ModelArgs: \n",
    "    dim: int\n",
    "    n_layers: int\n",
    "    head_dim: int\n",
    "    hidden_dim: int\n",
    "    n_heads: int \n",
    "    n_kv_hads: int \n",
    "    sliding_window: int \n",
    "    norm_eps: float \n",
    "    vocab_size: int \n",
    "    \n",
    "    max_batch_size: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(key: torch.Tensor, values: torch.Tensor, repeasts: int):\n",
    "    keys = torch.repeat_interleave(keys, repeasts=repeasts, dim=2)\n",
    "    values = torch.repeat_interleave(values, repeasts=repeasts, dim=2)\n",
    "    return keys, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unserstanding rotary embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    freqs_cis: complex - (seq_len, head_dim / 2)\n",
    "    x: complex - (bsz, seq_len, head_dim / 2)\n",
    "    \"\"\"\n",
    "    ndim = x.ndim\n",
    "    assert 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1]), ( \n",
    "        freqs_cis.shape,\n",
    "        (x.shape[1], x.shape[-1]),\n",
    "    )\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(\n",
    "        xq: torch.Tensor,\n",
    "        xk: torch.Tensor,\n",
    "        freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "\n",
    "    freqs_cis = _reshape_for_broadcast(freqs_cis, xq_)\n",
    "\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
